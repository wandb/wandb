// Code generated by command: go run gen.go -avx512 -out ../accum_vector_avx512_amd64.s -pkg xxh3. DO NOT EDIT.

#include "textflag.h"

DATA prime_avx512<>+0(SB)/8, $0x000000009e3779b1
DATA prime_avx512<>+8(SB)/8, $0x000000009e3779b1
DATA prime_avx512<>+16(SB)/8, $0x000000009e3779b1
DATA prime_avx512<>+24(SB)/8, $0x000000009e3779b1
DATA prime_avx512<>+32(SB)/8, $0x000000009e3779b1
DATA prime_avx512<>+40(SB)/8, $0x000000009e3779b1
DATA prime_avx512<>+48(SB)/8, $0x000000009e3779b1
DATA prime_avx512<>+56(SB)/8, $0x000000009e3779b1
GLOBL prime_avx512<>(SB), RODATA|NOPTR, $64

// func accumAVX512(acc *[8]uint64, data *byte, key *byte, len uint64)
// Requires: AVX, AVX512F, MMX+
TEXT ·accumAVX512(SB), NOSPLIT, $0-32
	MOVQ      acc+0(FP), AX
	MOVQ      data+8(FP), CX
	MOVQ      key+16(FP), DX
	MOVQ      len+24(FP), BX
	VMOVDQU64 (AX), Z1
	VMOVDQU64 prime_avx512<>+0(SB), Z0
	VMOVDQU64 (DX), Z5
	VMOVDQU64 8(DX), Z6
	VMOVDQU64 16(DX), Z7
	VMOVDQU64 24(DX), Z8
	VMOVDQU64 32(DX), Z9
	VMOVDQU64 40(DX), Z10
	VMOVDQU64 48(DX), Z11
	VMOVDQU64 56(DX), Z12
	VMOVDQU64 64(DX), Z13
	VMOVDQU64 72(DX), Z14
	VMOVDQU64 80(DX), Z15
	VMOVDQU64 88(DX), Z16
	VMOVDQU64 96(DX), Z17
	VMOVDQU64 104(DX), Z18
	VMOVDQU64 112(DX), Z19
	VMOVDQU64 120(DX), Z20
	VMOVDQU64 128(DX), Z21
	VMOVDQU64 121(DX), Z22

accum_large:
	CMPQ       BX, $0x00000400
	JLE        accum
	VMOVDQU64  (CX), Z23
	VMOVDQU64  64(CX), Z4
	PREFETCHT0 1024(CX)
	PREFETCHT0 1088(CX)
	VPXORD     Z5, Z23, Z2
	VPXORD     Z6, Z4, Z3
	VPSHUFD    $0x31, Z2, Z24
	VPSHUFD    $0x31, Z3, Z25
	VPMULUDQ   Z2, Z24, Z2
	VPMULUDQ   Z3, Z25, Z3
	VPSHUFD    $0x4e, Z23, Z23
	VPSHUFD    $0x4e, Z4, Z4
	VPADDQ     Z1, Z23, Z1
	VMOVDQU64  128(CX), Z23
	VMOVDQU64  192(CX), Z26
	PREFETCHT0 1152(CX)
	PREFETCHT0 1216(CX)
	VPXORD     Z7, Z23, Z24
	VPXORD     Z8, Z26, Z27
	VPSHUFD    $0x31, Z24, Z25
	VPSHUFD    $0x31, Z27, Z28
	VPMULUDQ   Z24, Z25, Z24
	VPMULUDQ   Z27, Z28, Z27
	VPSHUFD    $0x4e, Z23, Z23
	VPSHUFD    $0x4e, Z26, Z26
	VPADDQ     Z2, Z24, Z2
	VPADDQ     Z3, Z27, Z3
	VPADDQ     Z4, Z26, Z4
	VPADDQ     Z1, Z23, Z1
	VMOVDQU64  256(CX), Z23
	VMOVDQU64  320(CX), Z26
	PREFETCHT0 1280(CX)
	PREFETCHT0 1344(CX)
	VPXORD     Z9, Z23, Z24
	VPXORD     Z10, Z26, Z27
	VPSHUFD    $0x31, Z24, Z25
	VPSHUFD    $0x31, Z27, Z28
	VPMULUDQ   Z24, Z25, Z24
	VPMULUDQ   Z27, Z28, Z27
	VPSHUFD    $0x4e, Z23, Z23
	VPSHUFD    $0x4e, Z26, Z26
	VPADDQ     Z2, Z24, Z2
	VPADDQ     Z3, Z27, Z3
	VPADDQ     Z4, Z26, Z4
	VPADDQ     Z1, Z23, Z1
	VMOVDQU64  384(CX), Z23
	VMOVDQU64  448(CX), Z26
	PREFETCHT0 1408(CX)
	PREFETCHT0 1472(CX)
	VPXORD     Z11, Z23, Z24
	VPXORD     Z12, Z26, Z27
	VPSHUFD    $0x31, Z24, Z25
	VPSHUFD    $0x31, Z27, Z28
	VPMULUDQ   Z24, Z25, Z24
	VPMULUDQ   Z27, Z28, Z27
	VPSHUFD    $0x4e, Z23, Z23
	VPSHUFD    $0x4e, Z26, Z26
	VPADDQ     Z2, Z24, Z2
	VPADDQ     Z3, Z27, Z3
	VPADDQ     Z4, Z26, Z4
	VPADDQ     Z1, Z23, Z1
	VMOVDQU64  512(CX), Z23
	VMOVDQU64  576(CX), Z26
	PREFETCHT0 1536(CX)
	PREFETCHT0 1600(CX)
	VPXORD     Z13, Z23, Z24
	VPXORD     Z14, Z26, Z27
	VPSHUFD    $0x31, Z24, Z25
	VPSHUFD    $0x31, Z27, Z28
	VPMULUDQ   Z24, Z25, Z24
	VPMULUDQ   Z27, Z28, Z27
	VPSHUFD    $0x4e, Z23, Z23
	VPSHUFD    $0x4e, Z26, Z26
	VPADDQ     Z2, Z24, Z2
	VPADDQ     Z3, Z27, Z3
	VPADDQ     Z4, Z26, Z4
	VPADDQ     Z1, Z23, Z1
	VMOVDQU64  640(CX), Z23
	VMOVDQU64  704(CX), Z26
	PREFETCHT0 1664(CX)
	PREFETCHT0 1728(CX)
	VPXORD     Z15, Z23, Z24
	VPXORD     Z16, Z26, Z27
	VPSHUFD    $0x31, Z24, Z25
	VPSHUFD    $0x31, Z27, Z28
	VPMULUDQ   Z24, Z25, Z24
	VPMULUDQ   Z27, Z28, Z27
	VPSHUFD    $0x4e, Z23, Z23
	VPSHUFD    $0x4e, Z26, Z26
	VPADDQ     Z2, Z24, Z2
	VPADDQ     Z3, Z27, Z3
	VPADDQ     Z4, Z26, Z4
	VPADDQ     Z1, Z23, Z1
	VMOVDQU64  768(CX), Z23
	VMOVDQU64  832(CX), Z26
	PREFETCHT0 1792(CX)
	PREFETCHT0 1856(CX)
	VPXORD     Z17, Z23, Z24
	VPXORD     Z18, Z26, Z27
	VPSHUFD    $0x31, Z24, Z25
	VPSHUFD    $0x31, Z27, Z28
	VPMULUDQ   Z24, Z25, Z24
	VPMULUDQ   Z27, Z28, Z27
	VPSHUFD    $0x4e, Z23, Z23
	VPSHUFD    $0x4e, Z26, Z26
	VPADDQ     Z2, Z24, Z2
	VPADDQ     Z3, Z27, Z3
	VPADDQ     Z4, Z26, Z4
	VPADDQ     Z1, Z23, Z1
	VMOVDQU64  896(CX), Z23
	VMOVDQU64  960(CX), Z26
	PREFETCHT0 1920(CX)
	PREFETCHT0 1984(CX)
	VPXORD     Z19, Z23, Z24
	VPXORD     Z20, Z26, Z27
	VPSHUFD    $0x31, Z24, Z25
	VPSHUFD    $0x31, Z27, Z28
	VPMULUDQ   Z24, Z25, Z24
	VPMULUDQ   Z27, Z28, Z27
	VPSHUFD    $0x4e, Z23, Z23
	VPSHUFD    $0x4e, Z26, Z26
	VPADDQ     Z2, Z24, Z2
	VPADDQ     Z3, Z27, Z3
	VPADDQ     Z4, Z26, Z4
	VPADDQ     Z1, Z23, Z1
	VPADDQ     Z1, Z2, Z1
	VPADDQ     Z3, Z4, Z3
	VPADDQ     Z1, Z3, Z1
	ADDQ       $0x00000400, CX
	SUBQ       $0x00000400, BX
	VPSRLQ     $0x2f, Z1, Z2
	VPTERNLOGD $0x96, Z1, Z21, Z2
	VPMULUDQ   Z0, Z2, Z1
	VPSHUFD    $0xf5, Z2, Z2
	VPMULUDQ   Z0, Z2, Z2
	VPSLLQ     $0x20, Z2, Z2
	VPADDQ     Z1, Z2, Z1
	JMP        accum_large

accum:
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z5, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z6, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z7, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z8, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z9, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z10, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z11, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z12, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z13, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z14, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z15, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z16, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z17, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z18, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z19, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX
	CMPQ      BX, $0x40
	JLE       finalize
	VMOVDQU64 (CX), Z0
	VPXORD    Z20, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1
	ADDQ      $0x00000040, CX
	SUBQ      $0x00000040, BX

finalize:
	CMPQ      BX, $0x00
	JE        return
	SUBQ      $0x40, CX
	ADDQ      BX, CX
	VMOVDQU64 (CX), Z0
	VPXORD    Z22, Z0, Z2
	VPSHUFD   $0x31, Z2, Z3
	VPMULUDQ  Z2, Z3, Z2
	VPSHUFD   $0x4e, Z0, Z0
	VPADDQ    Z1, Z2, Z1
	VPADDQ    Z1, Z0, Z1

return:
	VMOVDQU64 Z1, (AX)
	VZEROUPPER
	RET

// func accumBlockAVX512(acc *[8]uint64, data *byte, key *byte)
// Requires: AVX, AVX512F
TEXT ·accumBlockAVX512(SB), NOSPLIT, $0-24
	MOVQ       acc+0(FP), AX
	MOVQ       data+8(FP), CX
	MOVQ       key+16(FP), DX
	VMOVDQU64  (AX), Z1
	VMOVDQU64  prime_avx512<>+0(SB), Z0
	VMOVDQU64  (DX), Z2
	VMOVDQU64  8(DX), Z3
	VMOVDQU64  (CX), Z5
	VMOVDQU64  64(CX), Z4
	VPXORD     Z2, Z5, Z2
	VPXORD     Z3, Z4, Z3
	VPSHUFD    $0x31, Z2, Z6
	VPSHUFD    $0x31, Z3, Z7
	VPMULUDQ   Z2, Z6, Z2
	VPMULUDQ   Z3, Z7, Z3
	VPSHUFD    $0x4e, Z5, Z5
	VPSHUFD    $0x4e, Z4, Z4
	VPADDQ     Z1, Z5, Z1
	VMOVDQU64  16(DX), Z5
	VMOVDQU64  24(DX), Z6
	VMOVDQU64  128(CX), Z7
	VMOVDQU64  192(CX), Z8
	VPXORD     Z5, Z7, Z5
	VPXORD     Z6, Z8, Z9
	VPSHUFD    $0x31, Z5, Z6
	VPSHUFD    $0x31, Z9, Z10
	VPMULUDQ   Z5, Z6, Z5
	VPMULUDQ   Z9, Z10, Z9
	VPSHUFD    $0x4e, Z7, Z7
	VPSHUFD    $0x4e, Z8, Z8
	VPADDQ     Z2, Z5, Z2
	VPADDQ     Z3, Z9, Z3
	VPADDQ     Z4, Z8, Z4
	VPADDQ     Z1, Z7, Z1
	VMOVDQU64  32(DX), Z5
	VMOVDQU64  40(DX), Z6
	VMOVDQU64  256(CX), Z7
	VMOVDQU64  320(CX), Z8
	VPXORD     Z5, Z7, Z5
	VPXORD     Z6, Z8, Z9
	VPSHUFD    $0x31, Z5, Z6
	VPSHUFD    $0x31, Z9, Z10
	VPMULUDQ   Z5, Z6, Z5
	VPMULUDQ   Z9, Z10, Z9
	VPSHUFD    $0x4e, Z7, Z7
	VPSHUFD    $0x4e, Z8, Z8
	VPADDQ     Z2, Z5, Z2
	VPADDQ     Z3, Z9, Z3
	VPADDQ     Z4, Z8, Z4
	VPADDQ     Z1, Z7, Z1
	VMOVDQU64  48(DX), Z5
	VMOVDQU64  56(DX), Z6
	VMOVDQU64  384(CX), Z7
	VMOVDQU64  448(CX), Z8
	VPXORD     Z5, Z7, Z5
	VPXORD     Z6, Z8, Z9
	VPSHUFD    $0x31, Z5, Z6
	VPSHUFD    $0x31, Z9, Z10
	VPMULUDQ   Z5, Z6, Z5
	VPMULUDQ   Z9, Z10, Z9
	VPSHUFD    $0x4e, Z7, Z7
	VPSHUFD    $0x4e, Z8, Z8
	VPADDQ     Z2, Z5, Z2
	VPADDQ     Z3, Z9, Z3
	VPADDQ     Z4, Z8, Z4
	VPADDQ     Z1, Z7, Z1
	VMOVDQU64  64(DX), Z5
	VMOVDQU64  72(DX), Z6
	VMOVDQU64  512(CX), Z7
	VMOVDQU64  576(CX), Z8
	VPXORD     Z5, Z7, Z5
	VPXORD     Z6, Z8, Z9
	VPSHUFD    $0x31, Z5, Z6
	VPSHUFD    $0x31, Z9, Z10
	VPMULUDQ   Z5, Z6, Z5
	VPMULUDQ   Z9, Z10, Z9
	VPSHUFD    $0x4e, Z7, Z7
	VPSHUFD    $0x4e, Z8, Z8
	VPADDQ     Z2, Z5, Z2
	VPADDQ     Z3, Z9, Z3
	VPADDQ     Z4, Z8, Z4
	VPADDQ     Z1, Z7, Z1
	VMOVDQU64  80(DX), Z5
	VMOVDQU64  88(DX), Z6
	VMOVDQU64  640(CX), Z7
	VMOVDQU64  704(CX), Z8
	VPXORD     Z5, Z7, Z5
	VPXORD     Z6, Z8, Z9
	VPSHUFD    $0x31, Z5, Z6
	VPSHUFD    $0x31, Z9, Z10
	VPMULUDQ   Z5, Z6, Z5
	VPMULUDQ   Z9, Z10, Z9
	VPSHUFD    $0x4e, Z7, Z7
	VPSHUFD    $0x4e, Z8, Z8
	VPADDQ     Z2, Z5, Z2
	VPADDQ     Z3, Z9, Z3
	VPADDQ     Z4, Z8, Z4
	VPADDQ     Z1, Z7, Z1
	VMOVDQU64  96(DX), Z5
	VMOVDQU64  104(DX), Z6
	VMOVDQU64  768(CX), Z7
	VMOVDQU64  832(CX), Z8
	VPXORD     Z5, Z7, Z5
	VPXORD     Z6, Z8, Z9
	VPSHUFD    $0x31, Z5, Z6
	VPSHUFD    $0x31, Z9, Z10
	VPMULUDQ   Z5, Z6, Z5
	VPMULUDQ   Z9, Z10, Z9
	VPSHUFD    $0x4e, Z7, Z7
	VPSHUFD    $0x4e, Z8, Z8
	VPADDQ     Z2, Z5, Z2
	VPADDQ     Z3, Z9, Z3
	VPADDQ     Z4, Z8, Z4
	VPADDQ     Z1, Z7, Z1
	VMOVDQU64  112(DX), Z5
	VMOVDQU64  120(DX), Z6
	VMOVDQU64  896(CX), Z7
	VMOVDQU64  960(CX), Z8
	VPXORD     Z5, Z7, Z5
	VPXORD     Z6, Z8, Z9
	VPSHUFD    $0x31, Z5, Z6
	VPSHUFD    $0x31, Z9, Z10
	VPMULUDQ   Z5, Z6, Z5
	VPMULUDQ   Z9, Z10, Z9
	VPSHUFD    $0x4e, Z7, Z7
	VPSHUFD    $0x4e, Z8, Z8
	VPADDQ     Z2, Z5, Z2
	VPADDQ     Z3, Z9, Z3
	VPADDQ     Z4, Z8, Z4
	VPADDQ     Z1, Z7, Z1
	VMOVDQU64  128(DX), Z5
	VPADDQ     Z1, Z2, Z1
	VPADDQ     Z3, Z4, Z3
	VPADDQ     Z1, Z3, Z1
	VPSRLQ     $0x2f, Z1, Z2
	VPTERNLOGD $0x96, Z1, Z5, Z2
	VPMULUDQ   Z0, Z2, Z1
	VPSHUFD    $0xf5, Z2, Z2
	VPMULUDQ   Z0, Z2, Z2
	VPSLLQ     $0x20, Z2, Z2
	VPADDQ     Z1, Z2, Z1
	VMOVDQU64  Z1, (AX)
	VZEROUPPER
	RET
