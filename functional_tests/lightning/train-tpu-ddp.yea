id: 0.lightning.ddp.gpu
plugin:
  - wandb
tag:
  shard: service
  skip: True
command:
  program: train-tpu-ddp.py
depend:
  requirements:
    - torch
    - git+https://github.com/wandb/pytorch-lightning.git@wandb-service-attach
assert:
  - :wandb:runs_len: 1
  - :wandb:runs[0][config][some_hparam]: Logged Before Trainer starts DDP
  - :wandb:runs[0][summary][epoch]: 0
  - :wandb:runs[0][summary][trainer/global_step]: 0
  - :wandb:runs[0][exitcode]: 0
  - :op:contains:
    - :wandb:runs[0][telemetry][3]  # feature
    - 6  # grpc
  - :op:>:
    - :wandb:runs[0][summary][loss]
    - 0
  - :op:>:
    - :wandb:runs[0][summary][fake_test_acc]
    - 0
